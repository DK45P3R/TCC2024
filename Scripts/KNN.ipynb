#Modelo KNN

API calls

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import pickle

# Carregar os arquivos CSV
data_benign = pd.read_csv('api_calls_2b.csv', encoding='ISO-8859-1', sep=';', on_bad_lines='skip')
data_malign = pd.read_csv('api_calls_2m.csv', encoding='ISO-8859-1', sep=';', on_bad_lines='skip')

# Adicionar coluna de rótulo
data_malign['label'] = 1
data_benign['label'] = 0

# Remover a coluna "Nome do Arquivo" de cada dataset
data_benign = data_benign.drop(columns=['Nome do Arquivo'])
data_malign = data_malign.drop(columns=['Nome do Arquivo'])

# Concatenar os dados
data = pd.concat([data_benign, data_malign], ignore_index=True)

# Preencher valores faltantes com 0
data = data.fillna(0)

# Remover colunas não numéricas (como hashes, nomes de arquivos, etc.)
data = data.apply(pd.to_numeric, errors='coerce').fillna(0)

# Separar características e rótulos
X = data.drop('label', axis=1)  # Remove a coluna 'label'
y = data['label']

# Verificar e corrigir rótulos fora do esperado (0 e 1)
invalid_classes = set(np.unique(y)) - {0, 1}
if invalid_classes:
    print(f"Classes inválidas encontradas: {invalid_classes}")
    y = np.where(y > 1, 1, y)  # Corrigir valores > 1 para 1
    print(f"Classes corrigidas: {np.unique(y)}")

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Padronizar os dados com StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Definir o intervalo de valores para n_neighbors
param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 15, 20, 50, 75, 100]}

# Realizar busca em grid para encontrar o melhor n_neighbors
grid_search = GridSearchCV(KNeighborsClassifier(weights='distance'), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Melhor valor de n_neighbors encontrado
best_n_neighbors = grid_search.best_params_['n_neighbors']
print(f"\nMelhor valor de n_neighbors: {best_n_neighbors}")

# Usar o melhor modelo encontrado no GridSearch
knn_best = grid_search.best_estimator_

# Fazer previsões com o melhor modelo
y_pred = knn_best.predict(X_test)

# Gerar o relatório de classificação
report = classification_report(y_test, y_pred, target_names=['benigno', 'maligno'])
print("\nRelatório de Classificação:\n", report)

# Exibir a matriz de confusão
conf_matrix = confusion_matrix(y_test, y_pred)
print("Matriz de Confusão:\n", conf_matrix)

# Validar o modelo com validação cruzada (5-fold)
cv_scores = cross_val_score(knn_best, X, y, cv=5)
print("\nValidação Cruzada (5-fold) - Acurácia Média:", cv_scores.mean())

# Salvando o modelo treinado
arquivo_pickle = "KNN_API_best.sav"
pickle.dump(knn_best, open(arquivo_pickle, 'wb'))


Opcodes

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import pickle

# Carregar os arquivos CSV
data_benign = pd.read_csv('opcodes_2b.csv', encoding='ISO-8859-1', sep=';', on_bad_lines='skip')
data_malign = pd.read_csv('opcodes_2m.csv', encoding='ISO-8859-1', sep=';', on_bad_lines='skip')

# Adicionar coluna de rótulo
data_malign['label'] = 1
data_benign['label'] = 0

# Remover a coluna "Nome do Arquivo" de cada dataset
data_benign = data_benign.drop(columns=['Nome do Arquivo'])
data_malign = data_malign.drop(columns=['Nome do Arquivo'])

# Concatenar os dados
data = pd.concat([data_benign, data_malign], ignore_index=True)

# Preencher valores faltantes com 0
data = data.fillna(0)

# Remover colunas não numéricas (como hashes, nomes de arquivos, etc.)
data = data.apply(pd.to_numeric, errors='coerce').fillna(0)

# Separar características e rótulos
X = data.drop('label', axis=1)  # Remove a coluna 'label'
y = data['label']

# Verificar e corrigir rótulos fora do esperado (0 e 1)
invalid_classes = set(np.unique(y)) - {0, 1}
if invalid_classes:
    print(f"Classes inválidas encontradas: {invalid_classes}")
    y = np.where(y > 1, 1, y)  # Corrigir valores > 1 para 1
    print(f"Classes corrigidas: {np.unique(y)}")

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Padronizar os dados com StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Definir o intervalo de valores para n_neighbors
param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 15, 20, 50, 75, 100]}

# Realizar busca em grid para encontrar o melhor n_neighbors
grid_search = GridSearchCV(KNeighborsClassifier(weights='distance'), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Melhor valor de n_neighbors encontrado
best_n_neighbors = grid_search.best_params_['n_neighbors']
print(f"\nMelhor valor de n_neighbors: {best_n_neighbors}")

# Usar o melhor modelo encontrado no GridSearch
knn_best = grid_search.best_estimator_

# Fazer previsões com o melhor modelo
y_pred = knn_best.predict(X_test)

# Gerar o relatório de classificação
report = classification_report(y_test, y_pred, target_names=['benigno', 'maligno'])
print("\nRelatório de Classificação:\n", report)

# Exibir a matriz de confusão
conf_matrix = confusion_matrix(y_test, y_pred)
print("Matriz de Confusão:\n", conf_matrix)

# Validar o modelo com validação cruzada (5-fold)
cv_scores = cross_val_score(knn_best, X, y, cv=5)
print("\nValidação Cruzada (5-fold) - Acurácia Média:", cv_scores.mean())

# Salvando o modelo treinado
arquivo_pickle = "KNN_Op_test.sav"
pickle.dump(knn_best, open(arquivo_pickle, 'wb'))


Permissions

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import pickle

# Carregar os arquivos CSV
data_benign = pd.read_csv('permissions_2b.csv', encoding='ISO-8859-1', sep=';', on_bad_lines='skip')
data_malign = pd.read_csv('permissions_2m.csv', encoding='ISO-8859-1', sep=';', on_bad_lines='skip')

# Adicionar coluna de rótulo
data_malign['label'] = 1
data_benign['label'] = 0

# Remover a coluna "Nome do Arquivo" de cada dataset
data_benign = data_benign.drop(columns=['Nome do Arquivo'])
data_malign = data_malign.drop(columns=['Nome do Arquivo'])

# Concatenar os dados
data = pd.concat([data_benign, data_malign], ignore_index=True)

# Preencher valores faltantes com 0
data = data.fillna(0)

# Remover colunas não numéricas (como hashes, nomes de arquivos, etc.)
data = data.apply(pd.to_numeric, errors='coerce').fillna(0)

# Separar características e rótulos
X = data.drop('label', axis=1)  # Remove a coluna 'label'
y = data['label']

# Verificar e corrigir rótulos fora do esperado (0 e 1)
invalid_classes = set(np.unique(y)) - {0, 1}
if invalid_classes:
    print(f"Classes inválidas encontradas: {invalid_classes}")
    y = np.where(y > 1, 1, y)  # Corrigir valores > 1 para 1
    print(f"Classes corrigidas: {np.unique(y)}")

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Padronizar os dados com StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Definir o intervalo de valores para n_neighbors
param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 15, 20, 50, 75, 100]}

# Realizar busca em grid para encontrar o melhor n_neighbors
grid_search = GridSearchCV(KNeighborsClassifier(weights='distance'), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Melhor valor de n_neighbors encontrado
best_n_neighbors = grid_search.best_params_['n_neighbors']
print(f"\nMelhor valor de n_neighbors: {best_n_neighbors}")

# Usar o melhor modelo encontrado no GridSearch
knn_best = grid_search.best_estimator_

# Fazer previsões com o melhor modelo
y_pred = knn_best.predict(X_test)

# Gerar o relatório de classificação
report = classification_report(y_test, y_pred, target_names=['benigno', 'maligno'])
print("\nRelatório de Classificação:\n", report)

# Exibir a matriz de confusão
conf_matrix = confusion_matrix(y_test, y_pred)
print("Matriz de Confusão:\n", conf_matrix)

# Validar o modelo com validação cruzada (5-fold)
cv_scores = cross_val_score(knn_best, X, y, cv=5)
print("\nValidação Cruzada (5-fold) - Acurácia Média:", cv_scores.mean())

# Salvando o modelo treinado
arquivo_pickle = "KNN_P_best.sav"
pickle.dump(knn_best, open(arquivo_pickle, 'wb'))
